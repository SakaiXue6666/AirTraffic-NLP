"""
Date: 2021-05-31 19:50:58
LastEditors: GodK
"""

import os
import config_fly as config
import sys
import torch
import json
from transformers import BertTokenizerFast, BertModel, RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModelForCausalLM
from common.utils_lower import Preprocessor, multilabel_categorical_crossentropy
from models.Qwen_fly_scene_2_ner_test import DataMaker, MyDataset, MetricsCalculator
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import glob
import wandb
# from evaluate_2 import load_model
import time
import numpy as np
import matplotlib.pyplot as plt

from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

from torch.nn.utils.rnn import pad_sequence
# import torch

config = config.train_config
hyper_parameters = config["hyper_parameters"]

os.environ["TOKENIZERS_PARALLELISM"] = "true"
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
config["num_workers"] = 6 if sys.platform.startswith("linux") else 0

# for reproductivity
torch.manual_seed(hyper_parameters["seed"])  # pytorch random seed
# ÈÅøÂÖçÂõ†ÁÆóÊ≥ïÈÄâÊã©ÊàñÁ°¨‰ª∂‰ºòÂåñÂØºËá¥ÁöÑÈùûÁ°ÆÂÆöÊÄßË°å‰∏∫
torch.backends.cudnn.deterministic = True

if config["logger"] == "wandb" and config["run_type"] == "train":
    # init wandb
    wandb.init(project="GlobalPointer_" + config["exp_name"],
               config=hyper_parameters  # Initialize config
               )
    wandb.run.name = config["run_name"] + "_" + wandb.run.id

    model_state_dict_dir = wandb.run.dir
    logger = wandb
elif config["run_type"] == "train":
    model_state_dict_dir = os.path.join(config["path_to_save_model"], config["exp_name"],
                                        time.strftime("%Y-%m-%d_%H.%M.%S", time.localtime()))
    if not os.path.exists(model_state_dict_dir):
        os.makedirs(model_state_dict_dir)

tokenizer = AutoTokenizer.from_pretrained(config["qwen_path"], padding_side='left')
model = AutoModelForCausalLM.from_pretrained(
    config["qwen_path"],
    # quantization_config = quant_config,
    # device_map = "auto"
)

# 5. ËÆæÁΩÆ LoRA ÈÖçÁΩÆ
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "gate_proj", "down_proj"],  # ÂØπ Q/V Áü©ÈòµÂä† LoRA
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# 6. Â∞Ü LoRA Â∫îÁî®Âà∞Ê®°Âûã
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

model = model.to(device)


def load_data(data_path, data_type="train"):
    import json
    if data_type in ["train", "valid", "test"]:
        datas = []
        with open(data_path, encoding="utf-8") as f:
            first_char = f.read(1)
            f.seek(0)  # ÂõûÂà∞Êñá‰ª∂ÂºÄÂ§¥
            if first_char == "[":
                # Êñ∞Ê†ºÂºèÔºöJSONÊï∞ÁªÑ
                data_list = json.load(f)
            else:
                # ÊóßÊ†ºÂºèÔºöJSON lines
                data_list = [json.loads(line) for line in f]

        return data_list
    else:
        return json.load(open(data_path, encoding="utf-8"))


ent2id_path = os.path.join(config["data_home"], config["exp_name"], config["ent2id"])
ent2id = load_data(ent2id_path, "ent2id")
ent_type_size = len(ent2id)
id2ent = {v: k for k, v in ent2id.items()}

# Êú™ËÆ∞ÂΩïsceneÁ±ªÂûã
scene2id_path = os.path.join(config["data_home"], config["exp_name"], config["scene2id"])
scene2id = load_data(scene2id_path, "scene2id")
scene_type_size = len(scene2id)
id2scene = {v: k for k, v in scene2id.items()}

###
my_scene2id = load_data(scene2id_path, "scene2id")
my_scene2id["CLSn+1"] = 1000


def data_generator(data_type="train"):
    """
    ËØªÂèñÊï∞ÊçÆÔºåÁîüÊàêDataLoader„ÄÇ
    """

    if data_type == "train":
        train_data_path = os.path.join(config["data_home"], config["exp_name"], config["train_data"])
        train_data = load_data(train_data_path, "train")
        valid_data_path = os.path.join(config["data_home"], config["exp_name"], config["valid_data"])
        valid_data = load_data(valid_data_path, "valid")
        test_data_path = os.path.join(config["data_home"], config["exp_name"], config["test_data"])
        test_data = load_data(test_data_path, "test")
    elif data_type == "valid":
        valid_data_path = os.path.join(config["data_home"], config["exp_name"], config["valid_data"])
        valid_data = load_data(valid_data_path, "valid")
        train_data = []
    elif data_type == "test":
        test_data_path = os.path.join(config["data_home"], config["exp_name"], config["test_data"])
        test_data = load_data(test_data_path, "test")
        train_data = []

    all_data = train_data + valid_data + test_data #

    data_maker = DataMaker(tokenizer)

    if data_type == "train":
        train_dataloader = DataLoader(MyDataset(train_data),
                                      batch_size=hyper_parameters["batch_size"],
                                      shuffle=True,
                                      num_workers=config["num_workers"],
                                      drop_last=False,
                                      collate_fn=lambda x: data_maker.generate_batch(x, 512, ent2id, scene2id, data_type="train")
                                      )
        valid_dataloader = DataLoader(MyDataset(valid_data),
                                      batch_size=hyper_parameters["batch_size"],
                                      shuffle=False,
                                      num_workers=config["num_workers"],
                                      drop_last=False,
                                      collate_fn=lambda x: data_maker.generate_batch(x, 512, ent2id, scene2id, data_type="valid")
                                      )
        test_dataloader = DataLoader(MyDataset(test_data),  #
                                     batch_size=hyper_parameters["batch_size"],
                                     shuffle=False,
                                     num_workers=config["num_workers"],
                                     drop_last=False,
                                     collate_fn=lambda x: data_maker.generate_batch(x, 512, ent2id, my_scene2id, data_type="predict")
                                     )
        return train_dataloader, valid_dataloader, test_dataloader  #
    else:
        valid_dataloader = DataLoader(MyDataset(valid_data),
                                      batch_size=hyper_parameters["batch_size"],
                                      shuffle=True,
                                      num_workers=config["num_workers"],
                                      drop_last=False,
                                      collate_fn=lambda x: data_maker.generate_batch(x, 512, ent2id, scene2id)
                                      )
        return valid_dataloader


metrics = MetricsCalculator()


def train_step(batch_train, model, optimizer, criterion):
    (batch_samples,
     batch_input_ids, batch_attention_mask, _,
     batch_labels, _) = batch_train

    (batch_input_ids, batch_attention_mask, batch_labels) = (
        batch_input_ids.to(device), batch_attention_mask.to(device), batch_labels.to(device),
    )

    logits_scene = model(
        input_ids=batch_input_ids,
        attention_mask=batch_attention_mask,
        labels=batch_labels
    )

    loss = logits_scene.loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

if config["logger"] == "wandb" and config["run_type"] == "train":
    wandb.watch(model)


def train(model, dataloader, epoch, optimizer):
    model.train()

    # scheduler
    if hyper_parameters["scheduler"] == "CAWR":
        T_mult = hyper_parameters["T_mult"]
        rewarm_epoch_num = hyper_parameters["rewarm_epoch_num"]
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,
                                                                         len(train_dataloader) * rewarm_epoch_num,
                                                                         T_mult)
    elif hyper_parameters["scheduler"] == "Step":
        decay_rate = hyper_parameters["decay_rate"]
        decay_steps = hyper_parameters["decay_steps"]
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)
    else:
        scheduler = None

    pbar = tqdm(enumerate(dataloader), total=len(dataloader))
    total_loss = 0.
    for batch_ind, batch_data in pbar:

        loss = train_step(batch_data, model, optimizer, None)

        total_loss += loss

        avg_loss = total_loss / (batch_ind + 1)
        if scheduler is not None:
            scheduler.step()

        pbar.set_description(
            f'Project:{config["exp_name"]}, Epoch: {epoch + 1}/{hyper_parameters["epochs"]}, Step: {batch_ind + 1}/{len(dataloader)}')
        pbar.set_postfix(loss=avg_loss, lr=optimizer.param_groups[0]["lr"])

        if config["logger"] == "wandb" and batch_ind % config["log_interval"] == 0:
            logger.log({
                "epoch": epoch,
                "train_loss": avg_loss,
                "learning_rate": optimizer.param_groups[0]['lr'],
            })

    return avg_loss


def valid_step(batch_valid, model, data_type="valid"):
    (batch_samples,
     batch_input_ids, batch_attention_mask, _,
     _, true_texts) = batch_valid

    (batch_input_ids, batch_attention_mask) = (
        batch_input_ids.to(device), batch_attention_mask.to(device)
    )

    with torch.no_grad():
        scene_ids = model.generate(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_mask,
            max_new_tokens=64,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id
        )

    # # Ëé∑ÂèñÊØè‰∏™Ê†∑Êú¨ÁöÑ prompt ÂÆûÈôÖÈïøÂ∫¶ÔºàÈùû padding ÁöÑ token Êï∞Ôºâ
    # prompt_lengths = batch_attention_mask.sum(dim=1)  # shape: (batch_size,)
    # # ÂéªÊéâ promptÔºåÂè™‰øùÁïô response
    # output_ids = []
    # for i, prompt_len in enumerate(prompt_lengths):
    #     output_ids.append(scene_ids[i, prompt_len:])  # ‰ªé prompt_len ÂºÄÂßãÊà™Âèñ

    # ÂÅáËÆæ batch_input_ids.shape = (batch_size, seq_len)
    prompt_len = batch_input_ids.shape[1]
    # ÂéªÊéâ promptÔºåÂè™‰øùÁïô response
    output_ids = [scene_ids[i, prompt_len:] for i in range(scene_ids.size(0))]

    # padded Âà∞Áõ∏ÂêåÈïøÂ∫¶ÔºàÂèØÈÄâÔºå‰æãÂ¶Ç‰∏∫‰∫Ü batch_decodeÔºâ
    output_ids = pad_sequence(output_ids, batch_first=True, padding_value=tokenizer.pad_token_id)

    predict_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    predict_labels = [predict_text.split("assistant\n")[1].split("„ÄÅ") if ("assistant\n" in predict_text) else predict_text.split("„ÄÅ") for predict_text in predict_texts]

    if data_type == "test":
        print(predict_labels)
        return

    true_labels = [true_text.split("„ÄÅ") for true_text in true_texts]  ###
    f1_scene, p_scene, r_scene = metrics.get_scene_metrics(predict_labels, true_labels, scene_type_size, scene2id)

    return {
        "metrics": {
            # "ner": {"f1": f1_ner, "p": p_ner, "r": r_ner},
            "scene": {"f1": f1_scene, "p": p_scene, "r": r_scene}
        },
        # "ner_preds": ner_preds,
        # "ner_trues": ner_trues,
        "scene_preds": predict_labels,
        "scene_trues": true_labels,
        "samples": batch_samples
    }


def valid(model, dataloader, test_dataloader):
    model.eval()

    total_scene_f1 = total_scene_p = total_scene_r = 0.

    print_count = 0

    for batch_data in tqdm(dataloader, desc="Validating"):
        result = valid_step(batch_data, model)
        metrics_result = result["metrics"]

        total_scene_f1 += metrics_result["scene"]["f1"]
        total_scene_p += metrics_result["scene"]["p"]
        total_scene_r += metrics_result["scene"]["r"]

        # üîç ÊâìÂç∞È¢ÑÊµã vs ÁúüÂÆû
        # for i, sample in enumerate(result["samples"]):
        #     print("\n================= Sample =================")
        #     print("Text:", sample["text"])
        #
        #     # 1. ÊâìÂç∞ scene ÂàÜÁ±ª
        #     pred_scene_label_ids = result["scene_preds"][i]
        #     true_scene_label_ids = result["scene_trues"][i]
        #
        #     pred_scenes = [id2scene[j] for j, v in enumerate(pred_scene_label_ids) if v == 1] if id2scene else pred_scene_label_ids.tolist()
        #     true_scenes = [id2scene[j] for j, v in enumerate(true_scene_label_ids) if v == 1] if id2scene else true_scene_label_ids.tolist()
        #
        #     print("[Scene]")
        #     print("ÁúüÂÆûÊ†áÁ≠æ:", true_scenes)
        #     print("È¢ÑÊµãÊ†áÁ≠æ:", pred_scenes)
        #
        #     # 2. ÊâìÂç∞ NER ËØÜÂà´
        #     if id2ent:
        #         ner_pred_matrix = result["ner_preds"][i]
        #         ner_true_matrix = result["ner_trues"][i]
        #         pred_entities = decode_ent(sample["text"], ner_pred_matrix, tokenizer, 0)
        #         true_entities = decode_ent(sample["text"], ner_true_matrix, tokenizer, 0)
        #         print("[NER]")
        #         print("ÁúüÂÆûÂÆû‰Ωì:", true_entities)
        #         print("È¢ÑÊµãÂÆû‰Ωì:", pred_entities)
        #
        #     print("=========================================")
        #     print_count += 1

    n = len(dataloader)

    avg_scene_f1 = total_scene_f1 / n
    avg_scene_p = total_scene_p / n
    avg_scene_r = total_scene_r / n

    print("\n***************** Evaluation Results *****************")
    # print("[NER]")
    # print(f'Precision: {avg_ner_p:.4f}, Recall: {avg_ner_r:.4f}, F1: {avg_ner_f1:.4f}')
    print("[Scene]")
    print(f'Precision: {avg_scene_p:.4f}, Recall: {avg_scene_r:.4f}, F1: {avg_scene_f1:.4f}')
    print("******************************************************")

    ###
    for batch_data in tqdm(test_dataloader, desc="Validating"):
        valid_step(batch_data, model, data_type="test")

    return {
        # "ner_f1": avg_ner_f1,
        "scene_f1": avg_scene_f1
    }

def decode_ent(text, pred_matrix, tokenizer, threshold=0):
    # print(text)
    token2char_span_mapping = tokenizer(text, return_offsets_mapping=True)["offset_mapping"]
    id2ent = {id: ent for ent, id in ent2id.items()}
    pred_matrix = pred_matrix.cpu().numpy()
    ent_list = {}

    binary_matrix = (pred_matrix > threshold).astype(float)

    for ent_type_id, token_start_index, token_end_index in zip(*np.where(pred_matrix > threshold)):
        ent_type = id2ent[ent_type_id]
        ent_char_span = [token2char_span_mapping[token_start_index][0], token2char_span_mapping[token_end_index][1]]
        ent_text = text[ent_char_span[0]:ent_char_span[1]]

        ent_type_dict = ent_list.get(ent_type, {})
        ent_text_list = ent_type_dict.get(ent_text, [])
        ent_text_list.append(ent_char_span)
        ent_type_dict.update({ent_text: ent_text_list})
        ent_list.update({ent_type: ent_type_dict})
    # print(ent_list)
    return ent_list

# Êñ∞Â¢ûÁªòÂõæÂáΩÊï∞
def plot_epoch_loss_curve(epoch_losses, save_dir="loss_plots"):
    os.makedirs(save_dir, exist_ok=True)
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label='Avg Loss per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Curve Across Epochs')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, "loss_per_epoch.png"))
    plt.close()

    print(f"Loss curve saved to {os.path.join(save_dir, 'loss_per_epoch.png')}")

if __name__ == '__main__':
    if config["run_type"] == "train":
        train_dataloader, valid_dataloader, test_dataloader = data_generator()

        # optimizer
        init_learning_rate = float(hyper_parameters["lr"])
        optimizer = torch.optim.Adam(model.parameters(), lr=init_learning_rate)

        max_avg_f1 = 0.0

        all_epoch_losses = []
        for epoch in range(hyper_parameters["epochs"]):
            avg_loss = train(model, train_dataloader, epoch, optimizer)
            all_epoch_losses.append(avg_loss)

            valid_results = valid(model, valid_dataloader, test_dataloader)  # ËøîÂõû dict
            # valid_ner_f1 = valid_results["ner_f1"]
            valid_scene_f1 = valid_results["scene_f1"]
            # avg_f1 = (valid_ner_f1 + valid_scene_f1) / 2
            avg_f1 = valid_scene_f1

            if avg_f1 > max_avg_f1:
                max_avg_f1 = avg_f1
                if avg_f1 > config["f1_2_save"]:  # Âà§Êñ≠ÊòØÂê¶ÈúÄË¶Å‰øùÂ≠ò
                    model_state_num = len(glob.glob(model_state_dict_dir + "/model_state_dict_*.pt"))
                    #torch.save(model.state_dict(),
                               #os.path.join(model_state_dict_dir, f"model_state_dict_{model_state_num}.pt"))
                    model.save_pretrained(os.path.join(model_state_dict_dir, f"lora_model_{model_state_num}_{avg_f1}"))
                    tokenizer.save_pretrained(os.path.join(model_state_dict_dir, f"lora_tokenizer_{model_state_num}_{avg_f1}"))

            print(f"[Epoch {epoch}] Best Avg F1: {max_avg_f1:.4f}")
            print("******************************************")

            if config.get("logger", "") == "wandb":
                logger.log({
                    # "valid_ner_f1": valid_ner_f1,
                    "valid_scene_f1": valid_scene_f1,
                    "valid_avg_f1": avg_f1,
                    "best_avg_f1": max_avg_f1
                })

            plot_epoch_loss_curve(all_epoch_losses)